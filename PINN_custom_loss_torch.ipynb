{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-05T07:47:37.243077900Z",
     "start_time": "2023-10-05T07:47:36.249204Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing cv2: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msubprocess\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\conda-torch\\Lib\\site-packages\\cv2\\__init__.py:181\u001B[0m\n\u001B[0;32m    176\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m DEBUG: \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExtra Python code for\u001B[39m\u001B[38;5;124m\"\u001B[39m, submodule, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis loaded\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m DEBUG: \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOpenCV loader: DONE\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 181\u001B[0m \u001B[43mbootstrap\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\conda-torch\\Lib\\site-packages\\cv2\\__init__.py:153\u001B[0m, in \u001B[0;36mbootstrap\u001B[1;34m()\u001B[0m\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m DEBUG: \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRelink everything from native cv2 module to cv2 package\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    151\u001B[0m py_module \u001B[38;5;241m=\u001B[39m sys\u001B[38;5;241m.\u001B[39mmodules\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcv2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 153\u001B[0m native_module \u001B[38;5;241m=\u001B[39m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcv2\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    155\u001B[0m sys\u001B[38;5;241m.\u001B[39mmodules[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcv2\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m py_module\n\u001B[0;32m    156\u001B[0m \u001B[38;5;28msetattr\u001B[39m(py_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_native\u001B[39m\u001B[38;5;124m\"\u001B[39m, native_module)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\conda-torch\\Lib\\importlib\\__init__.py:126\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m    124\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    125\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 126\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mImportError\u001B[0m: DLL load failed while importing cv2: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import subprocess\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "import torchvision.models as models\n",
    "from unet_model import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d875ff9256c4403",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check pytorch versions and CPU/GPU availability\n",
    "print(torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU available\")\n",
    "    print(\"Num GPUs available: \", torch.cuda.device_count())\n",
    "else:\n",
    "    print(\"No GPU available\")\n",
    "\n",
    "command = \"nvcc --version\"\n",
    "result = subprocess.run(command, stdout=subprocess.PIPE, shell=True, text=True)\n",
    "print(result.stdout)\n",
    "torch.set_default_dtype(torch.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1903d0ad79c8d2e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,data_name,dir_name,num_in,im_dim,cropsize,frames,bg_lvl):\n",
    "        self.data_name = data_name\n",
    "        self.dir_name = dir_name\n",
    "        self.num_in = num_in\n",
    "        self.im_dim = im_dim\n",
    "        self.cropsize = cropsize\n",
    "        self.frames = frames\n",
    "        self.bg_lvl = bg_lvl\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_in\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        input_frames=np.zeros([self.cropsize,self.cropsize,self.frames])\n",
    "        gt_frames=np.zeros([self.cropsize,self.cropsize,1])\n",
    "        lr_frames=np.zeros([self.cropsize,self.cropsize,1])\n",
    "        patterns=np.zeros([self.cropsize,self.cropsize,self.frames])\n",
    "        center_x=(im_dim-cropsize)//2\n",
    "        center_y=(im_dim-cropsize)//2\n",
    "        \n",
    "        for j in range(1,(self.frames+1)):\n",
    "            input_path = os.path.join(self.dir_name,self.data_name,'input_frames',str(idx+1)+'_'+str(j)+'.png')\n",
    "            input_temp = cv2.imread(input_path,0) # type: ignore\n",
    "            input_temp = cv2.resize(input_temp,dsize=(self.im_dim,self.im_dim), interpolation=cv2.INTER_CUBIC)\n",
    "            input_temp = input_temp[center_y:center_y+cropsize, center_x:center_x+cropsize]\n",
    "            input_frames[:,:,j-1] = input_temp\n",
    "            \n",
    "        gt_path=os.path.join(self.dir_name,self.data_name,'ground_truth',str(idx+1)+'.png')\n",
    "        gt_temp = cv2.imread(gt_path,0)\n",
    "        gt_temp=cv2.resize(gt_temp,dsize=(self.im_dim,self.im_dim), interpolation=cv2.INTER_CUBIC)\n",
    "        gt_temp = gt_temp.reshape([self.im_dim,self.im_dim,1])\n",
    "        gt_temp = gt_temp[center_y:center_y+cropsize, center_x:center_x+cropsize]\n",
    "        gt_frames[:,:,:]=gt_temp\n",
    "        \n",
    "        lr_path=os.path.join(self.dir_name,self.data_name,'low_res',str(idx+1)+'.png')\n",
    "        lr_temp = cv2.imread(lr_path,0)\n",
    "        lr_temp=cv2.resize(lr_temp,dsize=(self.im_dim,self.im_dim), interpolation=cv2.INTER_CUBIC)\n",
    "        lr_temp = lr_temp.reshape([self.im_dim,self.im_dim,1])\n",
    "        lr_temp = lr_temp[center_y:center_y+cropsize, center_x:center_x+cropsize]\n",
    "        lr_frames[:,:,:]=lr_temp\n",
    "        \n",
    "        psf = cv2.imread(dir_name+data_name+'/psf.png',0)\n",
    "        psf = psf.reshape([np.ma.size(psf,0),np.ma.size(psf,0),1])\n",
    "        \n",
    "        for j in range(1,(self.frames+1)):\n",
    "            pattern_path = os.path.join(self.dir_name,self.data_name,'patterns',str(j)+'.png')\n",
    "            pattern_temp = cv2.imread(pattern_path,0)\n",
    "            pattern_temp = cv2.resize(pattern_temp,dsize=(self.im_dim,self.im_dim), interpolation=cv2.INTER_CUBIC) \n",
    "            pattern_temp = pattern_temp[center_y:center_y+cropsize, center_x:center_x+cropsize]\n",
    "            patterns[:,:,j-1] = pattern_temp\n",
    "        \n",
    "        sample = {\n",
    "            'input_frames':self.transform(input_frames),\n",
    "            'gt_frames':self.transform(gt_frames),\n",
    "            'lr_frames':self.transform(lr_frames),\n",
    "            'patterns':self.transform(patterns),\n",
    "            'psf':self.transform(psf)\n",
    "        }\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "# Usage\n",
    "data_name='microtubules'\n",
    "dir_name='Data/SIM/'\n",
    "num_in=50\n",
    "im_dim=480\n",
    "cropsize=480\n",
    "frames=9\n",
    "bg_lvl=0\n",
    "\n",
    "custom_dataset=CustomDataset(data_name,dir_name,num_in,im_dim,cropsize,frames,bg_lvl)\n",
    "data_loader=DataLoader(dataset=custom_dataset,batch_size=1,shuffle=True)\n",
    "\n",
    "ind = 23\n",
    "sample=custom_dataset[ind-1]\n",
    "input_frames=sample['input_frames']\n",
    "gt_frames=sample['gt_frames']\n",
    "lr_frames=sample['lr_frames']\n",
    "patterns=sample['patterns']\n",
    "psf=sample['psf']\n",
    "\n",
    "#normalize data\n",
    "input_frames-=bg_lvl\n",
    "input_frames[input_frames<0]=0\n",
    "input_frames=input_frames/torch.max(input_frames.flatten()).item()\n",
    "gt_frames=gt_frames/torch.max(gt_frames.flatten()).item()\n",
    "lr_frames=lr_frames/torch.max(lr_frames.flatten()).item()\n",
    "patterns=patterns/torch.max(patterns.flatten()).item()\n",
    "print(torch.max(input_frames.flatten()).item())\n",
    "print(torch.max(gt_frames.flatten()).item())\n",
    "print(torch.max(lr_frames.flatten()).item())\n",
    "print(torch.max(patterns.flatten()).item())\n",
    "print(input_frames.shape)\n",
    "print(gt_frames.shape)\n",
    "print(lr_frames.shape)\n",
    "print(patterns.shape)\n",
    "print(psf.shape)\n",
    "# print(patterns.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b10942b79e1076",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_0=input_frames[0,:,:].numpy()\n",
    "plt.imshow(input_0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabeb543e4651ae",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class UNet(nn.Module):\n",
    "#     def __init__(self, k_size=3):\n",
    "#         super(UNet, self).__init__()\n",
    "# \n",
    "#         # Downward path\n",
    "#         self.conv1 = nn.Conv2d(9, 32, kernel_size=k_size, padding=1)  # Use padding=1 for 'same' padding\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.conv2 = nn.Conv2d(32, 32, kernel_size=k_size, padding=1)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.pool1 = nn.MaxPool2d(2)\n",
    "# \n",
    "#         self.conv4 = nn.Conv2d(32, 64, kernel_size=k_size, padding=1)\n",
    "#         self.relu4 = nn.ReLU()\n",
    "#         self.conv5 = nn.Conv2d(64, 64, kernel_size=k_size, padding=1)\n",
    "#         self.relu5 = nn.ReLU()\n",
    "#         self.pool2 = nn.MaxPool2d(2)\n",
    "# \n",
    "#         self.conv6 = nn.Conv2d(64, 128, kernel_size=k_size, padding=1)\n",
    "#         self.relu6 = nn.ReLU()\n",
    "#         self.conv7 = nn.Conv2d(128, 128, kernel_size=k_size, padding=1)\n",
    "#         self.relu7 = nn.ReLU()\n",
    "# \n",
    "#         # Upward path\n",
    "#         self.conv8 = nn.Conv2d(128, 256, kernel_size=k_size, padding=1)\n",
    "#         self.relu8 = nn.ReLU()\n",
    "#         self.conv9 = nn.Conv2d(256, 256, kernel_size=k_size, padding=1)\n",
    "#         self.relu9 = nn.ReLU()\n",
    "#         self.up1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "# \n",
    "#         self.conv10 = nn.Conv2d(128 + 128, 128, kernel_size=k_size, padding=1)\n",
    "#         self.relu10 = nn.ReLU()\n",
    "#         self.conv11 = nn.Conv2d(128, 128, kernel_size=k_size, padding=1)\n",
    "#         self.relu11 = nn.ReLU()\n",
    "#         self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "# \n",
    "#         self.conv12 = nn.Conv2d(64 + 64, 64, kernel_size=k_size, padding=1)\n",
    "#         self.relu12 = nn.ReLU()\n",
    "#         self.conv13 = nn.Conv2d(64, 64, kernel_size=k_size, padding=1)\n",
    "#         self.relu13 = nn.ReLU()\n",
    "#         self.up3 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "# \n",
    "#         self.conv14 = nn.Conv2d(32 + 32, 32, kernel_size=k_size, padding=1)\n",
    "#         self.relu14 = nn.ReLU()\n",
    "#         self.conv15 = nn.Conv2d(32, 32, kernel_size=k_size, padding=1)\n",
    "#         self.relu15 = nn.ReLU()\n",
    "#         self.conv16 = nn.Conv2d(32, 32, kernel_size=k_size, padding=1)\n",
    "#         self.relu16 = nn.ReLU()\n",
    "# \n",
    "#         self.output = nn.Conv2d(32, 1, kernel_size=1, padding=1)\n",
    "# \n",
    "#     def forward(self, x):\n",
    "#         # Downward path\n",
    "#         x1 = self.conv1(x)\n",
    "#         x1 = self.relu1(x1)\n",
    "#         x1 = self.conv2(x1)\n",
    "#         x1 = self.relu2(x1)\n",
    "#         x1 = self.conv3(x1)\n",
    "#         x1 = self.relu3(x1)\n",
    "#         down1 = self.pool1(x1)\n",
    "# \n",
    "#         x2 = self.conv4(down1)\n",
    "#         x2 = self.relu4(x2)\n",
    "#         x2 = self.conv5(x2)\n",
    "#         x2 = self.relu5(x2)\n",
    "#         down2 = self.pool2(x2)\n",
    "# \n",
    "#         x3 = self.conv6(down2)\n",
    "#         x3 = self.relu6(x3)\n",
    "#         x3 = self.conv7(x3)\n",
    "#         x3 = self.relu7(x3)\n",
    "#         down3 = self.pool3(x3)\n",
    "# \n",
    "#         # Upward path\n",
    "#         x4 = self.conv8(down3)\n",
    "#         x4 = self.relu8(x4)\n",
    "#         x4 = self.conv9(x4)\n",
    "#         x4 = self.relu9(x4)\n",
    "#         up1 = self.up1(x4)\n",
    "# \n",
    "#         cat1 = torch.cat([x3, up1], dim=1)\n",
    "#         x5 = self.conv10(cat1)\n",
    "#         x5 = self.relu10(x5)\n",
    "#         x5 = self.conv11(x5)\n",
    "#         x5 = self.relu11(x5)\n",
    "#         up2 = self.up2(x5)\n",
    "# \n",
    "#         cat2 = torch.cat([x2, up2], dim=1)\n",
    "#         x6 = self.conv12(cat2)\n",
    "#         x6 = self.relu12(x6)\n",
    "#         x6 = self.conv13(x6)\n",
    "#         x6 = self.relu13(x6)\n",
    "#         up3 = self.up3(x6)\n",
    "# \n",
    "#         cat3 = torch.cat([x1, up3], dim=1)\n",
    "#         x7 = self.conv14(cat3)\n",
    "#         x7 = self.relu14(x7)\n",
    "#         x7 = self.conv15(x7)\n",
    "#         x7 = self.relu15(x7)\n",
    "#         x7 = self.conv16(x7)\n",
    "#         x7 = self.relu16(x7)\n",
    "# \n",
    "#         output = self.output(x7)\n",
    "# \n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f60ee298b4f5418",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SSIM_loss\n",
    "def SSIM_Loss(y_true, y_pred):\n",
    "    ssim_loss = 1 - ssim(y_true, y_pred)\n",
    "    # print(\"ssim_loss:\",ssim_loss)\n",
    "    return ssim_loss\n",
    "\n",
    "# perceptual loss\n",
    "def perceptual_loss(in1, in2):\n",
    "    in1=vgg_normalize(in1)\n",
    "    in2=vgg_normalize(in2)\n",
    "    features1=vgg19(in1)\n",
    "    features2=vgg19(in2)\n",
    "    # print(\"size features1\",features1.element_size() * features1.nelement())\n",
    "    # print(\"size features2\",features2.element_size() * features2.nelement())\n",
    "    criterion=torch.nn.MSELoss()\n",
    "    perc_loss=criterion(features1,features2)\n",
    "    # print(\"perc_loss:\",perc_loss)\n",
    "    return perc_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e399c193cb8714ba",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Physics_loss(y_true, y_pred):\n",
    "    loss=torch.tensor(0.0,dtype=torch.float64).to(cuda)\n",
    "    # y_pred=torch.cat([y_pred]*frames,dim=1)\n",
    "    # print(\"y_pred shape:\",y_pred.shape)\n",
    "    # print(\"y_true shape:\",y_true.shape)\n",
    "    # print(\"patterns shape:\",patterns.shape)\n",
    "    for i in range(batch_size):\n",
    "        prod=y_pred*patterns\n",
    "        # print(\"prod shape:\",prod.shape)\n",
    "        conv=torch.zeros_like(prod)\n",
    "        # print(\"conv type\",conv.dtype)\n",
    "        padding = (psf.shape[-2] - 1) // 2\n",
    "        for j in range(frames):\n",
    "            single_image=prod[:,j:j+1,:,:]\n",
    "            # print(\"single image type: \",single_image.dtype)\n",
    "            # print(\"psf type: \",psf.dtype)\n",
    "            convolved_image=f.conv2d(single_image,psf,stride=1,padding=padding)\n",
    "            # print(\"convolved image shape:\",convolved_image.shape)\n",
    "            conv[:,j:j+1,:,:]=convolved_image\n",
    "        conv=(conv-torch.min(conv))/(torch.max(conv)-torch.min(conv))\n",
    "        # print(\"conv shape:\",conv.shape)\n",
    "        true=y_true\n",
    "        # individual_losses=torch.zeros(frames)\n",
    "        # # finding perceptual loss for each frame\n",
    "        # for j in range(frames):\n",
    "        #     y_true_frame=y_true[:,j:j+1,:,:]\n",
    "        #     y_pred_frame=conv[:,j:j+1,:,:]\n",
    "        #     y_true_rgb=torch.cat([y_true_frame]*3,dim=1)\n",
    "        #     y_pred_rgb=torch.cat([y_pred_frame]*3,dim=1)\n",
    "        #     # print(\"y_pred_rgb shape:\",y_true_rgb.shape)\n",
    "        #     # print(\"y_true_rgb shape:\",y_pred_rgb.shape)\n",
    "        #     pl_j_loss=perceptual_loss(y_true_rgb,y_pred_rgb)\n",
    "        #     # torch.cuda.empty_cache()\n",
    "        #     individual_losses[j]=pl_j_loss\n",
    "        # normalized_losses=individual_losses/frames\n",
    "        # final_pl=torch.sum(normalized_losses)\n",
    "        # print(\"final_pl:\",final_pl)\n",
    "\n",
    "        #total loss\n",
    "        im_loss=SSIM_Loss(conv,true)\n",
    "        loss+=im_loss\n",
    "        print(\"total_loss: \",loss)\n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93fe01a40ac19b1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = UNet(n_channels=9, n_classes=1)\n",
    "print(\"{} parameters in total\".format(sum(x.numel() for x in model.parameters())))\n",
    "learning_rate = 0.001\n",
    "\n",
    "cuda=torch.device('cuda:0')\n",
    "model=model.to(cuda)\n",
    "\n",
    "# Create the Adam optimizer with the initial learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create an ExponentialLR scheduler to decay the learning rate\n",
    "lr_schedule = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.90)\n",
    "\n",
    "# Set batch size and number of epochs\n",
    "batch_size = 1\n",
    "epochs = 50\n",
    "\n",
    "############################################################################################################\n",
    "ssim = StructuralSimilarityIndexMeasure(data_range=1.0, reduction='elementwise_mean').to(cuda)\n",
    "vgg19= models.vgg19(weights='VGG19_Weights.DEFAULT').features.to(cuda).eval()\n",
    "vgg_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).to(cuda)\n",
    "############################################################################################################\n",
    "\n",
    "l = Physics_loss\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2258873ffb70a152",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare 4D tensors for input from 3D tensors\n",
    "input_frames=input_frames.unsqueeze(0).to(cuda)\n",
    "gt_frames=gt_frames.unsqueeze(0).to(cuda)\n",
    "lr_frames=lr_frames.unsqueeze(0).to(cuda)\n",
    "patterns=patterns.unsqueeze(0).to(cuda)\n",
    "psf=psf.unsqueeze(0).to(cuda)\n",
    "print(\"input shape:\",input_frames.shape)\n",
    "print(\"gt shape:\",gt_frames.shape)\n",
    "print(\"lr shape:\",lr_frames.shape)\n",
    "print(\"patterns shape:\",patterns.shape)\n",
    "print(\"psf shape:\",psf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a4df8183ef7a2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_losses=[]\n",
    "torch.backends.cudnn.benchmark = True\n",
    "for epoch in range(epochs):\n",
    "    running_loss=0.0\n",
    "     # zero the parameter gradients\n",
    "    for param in model.parameters():\n",
    "        param.grad=None\n",
    "    outputs=model(input_frames) # forward pass\n",
    "    loss=l(input_frames,outputs)\n",
    "    running_loss+=loss\n",
    "    loss.backward() #backpropagate the loss\n",
    "    optimizer.step() # update the weights\n",
    "    lr_schedule.step() # update the learning rate\n",
    "    train_loss=running_loss/1\n",
    "    train_losses.append(train_loss)\n",
    "    #print average training loss for the epoch\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d348404063ffad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(epochs),train_losses, label='Optimization loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0756775e7b6d0f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output_tensor = model(input_frames)\n",
    "\n",
    "recon_image = output_tensor.detach().cpu().numpy()\n",
    "recon_image = recon_image[0, 0, :, :]  # Assuming a single image in the batch\n",
    "recon_image = recon_image[20:(im_dim-20), 20:(im_dim-20)]\n",
    "recon_image = (recon_image - np.amin(recon_image)) / (np.amax(recon_image) - np.amin(recon_image))\n",
    "\n",
    "# Convert gt_frames and lr_frames to NumPy arrays\n",
    "gt_frames = gt_frames.detach().cpu().numpy()\n",
    "lr_frames = lr_frames.detach().cpu().numpy()\n",
    "\n",
    "gt = gt_frames[0, 0, :, :]  # Assuming a single image in the batch\n",
    "gt = gt[20:(im_dim-20), 20:(im_dim-20)]\n",
    "gt = (gt - np.amin(gt)) / (np.amax(gt) - np.amin(gt))\n",
    "gt = gt.reshape((im_dim-40), (im_dim-40))\n",
    "\n",
    "lowres = lr_frames[0, 0, :, :]  # Assuming a single image in the batch\n",
    "lowres = lowres[20:(im_dim-20), 20:(im_dim-20)]\n",
    "lowres = (lowres - np.amin(lowres)) / (np.amax(lowres) - np.amin(lowres))\n",
    "lowres = lowres.reshape((im_dim-40), (im_dim-40))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(gt, cmap='inferno')\n",
    "plt.title('Ground truth image')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(recon_image, cmap='inferno')\n",
    "plt.title('PINN Result')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(lowres, cmap='inferno')\n",
    "plt.title('Diffraction limited image')\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
